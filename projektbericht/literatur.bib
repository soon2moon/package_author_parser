@article{Abdelkafi2019,
  author         = {Abdelkafi},
  title          = {Künstliche Intelligenz (KI) im Unternehmenskontext: Literaturanalyse und Thesenpapier},
  organization   = {Fraunhofer - Zentrum für internationales Management und wissensökonomie IMW},
  year           = {2019}
}

@article{cheung2024,
  author         = {Ming Cheung},
  title          = {A Reality check of the benefits of LLM in business},
  organization   = {Beta Labs, Lane Crawford Joyce Group},
  howpublished   = {Research Report},
  url            = {https://arxiv.org/pdf/2406.10249},
  year           = {2024},
  note           = {Acessed: January 23, 2025},
}

@misc{Fraunhofer,
  author         = {Thorsten Honroth, Dr. Julien Siebert und Patricia Kelbert},
  title          = {Retrieval Augmented Generation (RAG): Chatten mit den eigenen Daten},  title = {},
  organization   = {Fraunhofer IESE},
  year           = {2024},
  url            = {https://www.iese.fraunhofer.de/blog/retrieval-augmented-generation-rag/},
  note           = {Accessed: 2025-01-29}
}

@misc{Kelbert,
  author         = {Patricia Kelbert, Dr. Julien Siebert und Lisa Jöckel},
  title          = {Was sind Large Language Models? Und was ist bei der Nutzung von KI-Sprachmodellen zu beachten?},
  organization   = {Fraunhofer IESE},
  year           = {2024},
  url            = {https://www.iese.fraunhofer.de/blog/large-language-models-ki-sprachmodelle/},
  note           = {Accessed: 2025-01-29}
}

@article{Vaswani2017,
  author         = {Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin},
  title          = {Attention Is All You Need},
  journal        = {Curran Associates, Inc.},
  year           = {2017},
  url            = {https://arxiv.org/abs/1706.03762}
}

@misc{Wei,
  author         = {Chengwei Wei, Yun-Cheng Wang, Bin Wang, C.-C. Jay Kuo},
  title          = {An Overview on Language Models: Recent Developments and Outlook},
  howpublished   = {arXiv preprint},
  year           = {2023},
  url            = {https://arxiv.org/abs/2303.05759}
}

@misc{thamm,
  author         = {Alexander Thamm},
  title          = {Large Language Models - Eine Einführung},
  howpublished   = {Blogartikel auf alexanderthamm.com},
  year           = {2023},
  url            = {https://www.alexanderthamm.com/de/blog/large-language-models-eine-einfuehrung/}
}

@misc{Kaddour,
  author         = {Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert McHardy},
  title          = {Challenges and Applications of Large Language Models},
  howpublished   = {arXiv preprint},
  year           = {2023},
  url            = {https://arxiv.org/abs/2307.10169}
}

@misc{IWKoeln2022,
  author         = {Institut der deutschen Wirtschaft Köln},
  title          = {Parameter von KI-Modellen},
  howpublished   = {IW-Kurzbericht},
  year           = {2022},
  url            = {https://www.iwkoeln.de/fileadmin/user_upload/Studien/Kurzberichte/PDF/2022/IW-Kurzbericht_2022-Parameter-KI-Modelle.pdf}
}

@misc{forschungLehreKI,
  author         = {Forschung \& Lehre},
  title          = {Chancen einer KI-gestützten Wissenschaft},
  howpublished   = {Online-Artikel auf forschung-und-lehre.de},
  year           = {2023},
  url            = {https://www.forschung-und-lehre.de/forschung/chancen-einer-ki-gestuetzten-wissenschaft-6667}
}

@misc{Bergmann,
  author         = {Dave Bergmann},
  title          = {What is latent space?},
  url            = {https://www.ibm.com/think/topics/latent-space},
}

@misc{Tiu20,
  author        = {Ekin Tiu},
  title         = {Understanding Latent Space in Machine Learning},
  url           = {https://towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d?gi=7acb083387ad}
}

@misc{Rohrer20,
  author       = {Brandon Rohrer},
  title        = {Convolution in one dimension for neural networks},
  url          = {https://e2eml.school/convolution_one_d.html}
}

@misc{Kuchaiev,
  author       = {Oleksii Kuchaiev, Boris Ginsburg},
  title        = {Factorization tricks for LSTM networks},
  url          = {https://arxiv.org/abs/1703.10722}
}

@misc{Allamar,
  author       = {Jay Alammar},
  title        = {The Illustrated Transformer},
  url          = {https://jalammar.github.io/illustrated-transformer/}
}