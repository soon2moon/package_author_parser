{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from typing import List\n",
    "from loguru import logger\n",
    "from unittest import TestCase\n",
    "from unittest.mock import patch\n",
    "from pydantic import BaseModel, Field\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "from evaluations.core.metrics import *\n",
    "from author_parser.package_author_parser import *\n",
    "from evaluations.core.data_loader import load_data\n",
    "from evaluations.core.results_logger import log_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorName(BaseModel):\n",
    "    \"\"\"Name of an author, maintainer, contributor or copyright holder.\"\"\"\n",
    "    name: str = Field(description=\"Name of the individual\")\n",
    "\n",
    "class AuthorsWithName(BaseModel):\n",
    "    \"\"\"List of Authors\"\"\"\n",
    "    names: List[AuthorName] = Field(description=\"List of all the copyright holders, authors, maintainers, or contributors mentioned in the text\")\n",
    "\n",
    "class AuthorNameAndRole(BaseModel):\n",
    "    \"\"\"Name of an author, maintainer, contributor or copyright holder.\"\"\"\n",
    "    name: str = Field(description=\"Name of the individual\")\n",
    "    role: str = Field(description=\"Role of the individual. One of following: author, maintainer, contributor, or copyright holder. Default is author\")\n",
    "\n",
    "class AuthorsWithNameAndRole(BaseModel):\n",
    "    names: List[AuthorNameAndRole] = Field(description=\"List of all the copyright holders, authors, maintainers, or contributors mentioned in the text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestContextVariationEffects(TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.category = \"query-prompt-variation-effects\"\n",
    "        cls.author_parser = PackageParser()\n",
    "        cls.eval_documents = load_data(\"documents\")\n",
    "        cls.eval_chunk_params = load_data(\"chunk_params\") # [{ 'id': 'chunk_1', 'chunk_size': 512, 'chunk_overlap': 0 }, 'id': 'chunk2', 'chunk_size': 512, ...]\n",
    "        cls.eval_query_prompts = load_data(\"query_prompts\")\n",
    "        cls.eval_results = []\n",
    "\n",
    "        cls.documents = [\n",
    "                                Document(\n",
    "                                    id=test_doc[\"id\"],\n",
    "                                    text=test_doc[\"text\"],\n",
    "                                    metadata={\"file_name\": test_doc[\"file_name\"], \"category\": \"test\"}\n",
    "                                ) \n",
    "                                for test_doc in cls.eval_documents\n",
    "                              ]\n",
    "        cls.doc_ids = [doc[\"id\"] for doc in cls.eval_documents]\n",
    "        cls.expected_author_names = [name for test_doc in cls.eval_documents for name in test_doc[\"entities\"]]\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        log_results(cls.category, cls.eval_results)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def update_result(cls, test_case, doc_ids, duration, full_result, result, expected_result, score):\n",
    "      result = {\n",
    "                  \"category\": cls.category,\n",
    "                  \"eval_name\": test_case,\n",
    "                  \"llm_model_name\": DEFAULT_LLM_MODEL,\n",
    "                  \"embedding_model_name\": DEFAULT_EMBEDDING_MODEL,\n",
    "                  \"query\": cls.author_parser.query,\n",
    "                  \"chunk_size\": cls.author_parser.chunk_params[\"chunk_size\"],\n",
    "                  \"chunk_overlap\": cls.author_parser.chunk_params[\"chunk_overlap\"],\n",
    "                  \"prompt\": cls.author_parser.query_prompt_template_str,\n",
    "                  \"documents\": doc_ids,\n",
    "                  \"duration\": duration,\n",
    "                  \"result\": result,\n",
    "                  \"expected_result\": expected_result,\n",
    "                  \"full_result\": full_result,\n",
    "                  \"f1_score\": score[\"f1_score\"],\n",
    "                  \"precision\": score[\"precision\"],\n",
    "                  \"recall\": score[\"recall\"],\n",
    "                }\n",
    "\n",
    "      cls.eval_results.append(result)\n",
    "\n",
    "    @patch(\"author_parser.package_author_parser.Authors\", AuthorsWithName)\n",
    "    def test_extract_name_with_context_input(self):\n",
    "        # Given\n",
    "        prompt_template_id = \"Q2\"\n",
    "        author_parser = PackageParser()\n",
    "        query_prompt = [qp for qp in self.eval_query_prompts if qp[\"id\"] == prompt_template_id][0]\n",
    "        logger.info(query_prompt)\n",
    "\n",
    "        # When\n",
    "        author_parser.set_system_prompt_template(query_prompt)\n",
    "        result = author_parser.extract_authors_from_docs(self.documents)\n",
    "\n",
    "        # Then\n",
    "        full_result = str(result.response)\n",
    "        author_names = [author.name for author in result.response.names]\n",
    "\n",
    "        score = f1_score(author_names, self.expected_author_names)\n",
    "        self.update_result(query_prompt[\"type\"],\n",
    "                        author_parser,\n",
    "                        prompt_template_id,\n",
    "                        full_result,\n",
    "                        author_names,\n",
    "                        score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'query_prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test \u001b[38;5;241m=\u001b[39m TestContextVariationEffects()\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_extract_name_with_context_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_prompt\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'query_prompt'"
     ]
    }
   ],
   "source": [
    "test = TestContextVariationEffects()\n",
    "result = test.test_extract_name_with_context_input\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
